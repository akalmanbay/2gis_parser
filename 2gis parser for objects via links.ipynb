{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import csv\n",
    "from urllib.request import Request, urlopen\n",
    "import urllib\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "import time\n",
    "from selenium import webdriver\n",
    "import sys\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = {\"user-agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36\" ,'referer':'https://www.google.com/'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defined functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parser: object data collection\n",
    "def object_parser(object_request, subcat):\n",
    "    \n",
    "    item_soup = BeautifulSoup(object_request.text, 'lxml')\n",
    "    \n",
    "    try:\n",
    "        name = item_soup.find('span', class_=\"_oqoid\").text\n",
    "    except:\n",
    "        name = None\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        region = item_soup.find('div', class_=\"_1p8iqzw\").text\n",
    "    except:\n",
    "        region = None\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        type_ = item_soup.find('span', class_=\"_btwk9a2\").text\n",
    "    except:\n",
    "        type_ = None\n",
    "        pass\n",
    "        \n",
    "    try:\n",
    "        subtype = item_soup.find('div', class_=\"_11eqcnu\").text\n",
    "    except:\n",
    "        subtype = None\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        rating = item_soup.find('span', class_=\"_1n8h0vx\").text\n",
    "    except:\n",
    "        rating = None\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        ratingcount = item_soup.find('span', class_=\"_gg5kmr\").text\n",
    "    except:\n",
    "        ratingcount = None\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        coord_link = str(item_soup.find_all('meta')[1])\n",
    "        coord = re.findall(\"([0-9]+[,.]+[0-9]+)\",coord_link)\n",
    "    except:\n",
    "        coord = None\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        address = item_soup.find('span', class_=\"_er2xx9\").find('a', class_=\"_ke2cp9k\").text\n",
    "    except:\n",
    "        address = item_soup.find('span', class_=\"_er2xx9\").text.replace(u'\\u200b', '')\n",
    "        pass\n",
    "    \n",
    "    object_info = pd.DataFrame()\n",
    "    object_info = object_info.append({'Name':name ,'Region':region , 'Address':address, 'Type' :type_ , 'Subtype' :subtype, \\\n",
    "        'Coord' : coord, 'Rating' : rating, 'RatingCount' : ratingcount, 'Subcategory': subcat}, ignore_index = True)\n",
    "    return object_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for parsing object from a page with page links\n",
    "def get_df(cat_dict, cat_name):\n",
    "    i = 0\n",
    "    object_info = pd.DataFrame()\n",
    "    exception_links = []\n",
    "    for subcat, links in cat_dict.items():\n",
    "\n",
    "            for link in links:\n",
    "                i = i+1\n",
    "                try:\n",
    "                    #request            \n",
    "                    time.sleep(1.5)\n",
    "                    object_request = requests.get(link,headers=header)\n",
    "                    object_info = object_info.append(object_parser(object_request, str(subcat)))\n",
    "                except:\n",
    "                    print('Exception occured on iteration', i)\n",
    "                    exception_links.append(link)\n",
    "                    continue\n",
    "                if (i%100 == 0):\n",
    "                    print(i)\n",
    "                    object_info.to_csv(str(subcat)+'temp')\n",
    "    object_info['Category'] = str(cat_name)\n",
    "    print(\"End of the loop\")\n",
    "    return object_info, exception_links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing the objects from 2gis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categories: Subcategories\n",
    "#### Поесть: \n",
    "'Кафе', \"Рестораны\", \"Бары\", \"Быстрое питание\", \"Кофейни\",\n",
    "                  \"Готовые блюда\", \"Пиццерии\", \"Столовые\", \"Суши-бары\"\n",
    "\n",
    "#### Развлечения:\n",
    "Кинотеатры', \"Караоке\", \"Антикафе\", \"Музеи\", \"Библиотеки\",\"Театры\", \"Художественные выставки\", \"Парки культуры и отдыха\", \"Концертные залы\"\n",
    "\n",
    "#### Медицина: \n",
    "\"Аптеки\", \"Стоматологии\", \"Мед.центры\", \"Анализы\", \"Больницы\",\"Диагностические центры\", \"Детские специалисты\"\n",
    "\n",
    "#### Продукты: \n",
    "\"Супермаркеты\", \"Гипермаркеты\", \"Продуктовые магазины\"\n",
    "\n",
    "#### Спорт: \n",
    "\"Фитнес-клубы\",\"Тренажёрные залы\",\"Секции\",\"Бассейны\",\"Стадионы\"\n",
    "\n",
    "#### Образование: \n",
    "\"Гимназии\",\"Лицеи\",\"Лицей-интернаты\",\"Начальные школы\",\"Школы\",\"Школы-интернаты\",\"Детские сады\",\"Частные детские сады\",\"Развитие детей\",\"Колледжи\", \"Университеты\", \"Институты\",\"Академии\"\n",
    "\n",
    "#### Государственные учреждения:\n",
    "'Суды', \"Судебная и внесудебная экспертиза\",\"Отделения полиции\",\"Участковые пункты полиции\",\"ОМВД, УМВД, ГУМВД, МВД\",\"Прокуратуры\",\"Следственный комитет\", \"Цон\", \"Социальные службы\", \"Службы занятости\", \"Налоговые инспекции\", \"Законодательная власть\",\"Акимат города\", \"Маслихат города\", \"Акиматы поселений города\", \"Посольства, консульства\",\"Государственные службы\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# razvl_dict = np.load('razvl_links.npy',allow_pickle='TRUE').item()\n",
    "# df_razvl, exc_razvl = get_df(razvl_dict, 'Развлечения')\n",
    "# df_razvl.to_csv('df_razvl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sport_dict = np.load('sport_links.npy',allow_pickle='TRUE').item()\n",
    "# df_sport, exc_sport = get_df(sport_dict, 'Спорт')\n",
    "# df_sport.to_csv('df_sport')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# med_dict = np.load('med_links.npy',allow_pickle='TRUE').item()\n",
    "# df_med, exc_med = get_df(med_dict, 'Медицина')\n",
    "# df_med.to_csv('df_med')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# poest_dict = np.load('poest_links.npy',allow_pickle='TRUE').item()\n",
    "# df_poest, exc_poest = get_df(poest_dict, 'Поесть')\n",
    "# df_poest.to_csv('df_poest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prod_dict = np.load('prod_links.npy',allow_pickle='TRUE').item()\n",
    "# df_prod, exc_prod = get_df(prod_dict, 'Продукты')\n",
    "# df_prod.to_csv('df_prod')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# edu_dict = np.load('edu_links.npy',allow_pickle='TRUE').item()\n",
    "# df_edu, exc_dict = get_df(edu_dict, 'Образование')\n",
    "# df_edu.to_csv('df_edu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gov_dict = np.load('gov_links.npy',allow_pickle='TRUE').item()\n",
    "# df_gov, exc_gov = get_df(gov_dict, 'Государственные учреждения')\n",
    "# df_gov.to_csv('df_gov')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_edu = pd.read_csv('df_edu')\n",
    "df_razvl = pd.read_csv('df_razvl')\n",
    "df_sport = pd.read_csv('df_sport')\n",
    "df_gov = pd.read_csv('df_gov')\n",
    "df_prod = pd.read_csv('df_prod')\n",
    "df_poest = pd.read_csv('df_poest')\n",
    "df_med = pd.read_csv('df_med')\n",
    "\n",
    "gis2020 = pd.concat([df_edu,df_sport,df_razvl,df_gov, df_poest, df_prod, df_med])\n",
    "gis2020 = gis2020.drop(['Unnamed: 0'], axis= 1)[['Name','Address','Coord','Rating','RatingCount','Region',\n",
    "                                                  'Type', 'Subtype','Category','Subcategory']]\n",
    "gis2020.to_csv('gis2020.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
